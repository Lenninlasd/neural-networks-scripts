{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def SSE(y, y_hat):\n",
    "    #sum of the squared errors (SSE)\n",
    "    return 0.5*np.sum((y - y_hat)**2)\n",
    "\n",
    "def softmax(w, t = 1.0):\n",
    "    e = np.exp(np.array(w) / t)\n",
    "    dist = e / np.sum(e)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "records = 5000\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images[:records,:]\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)[:records,:]\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  784\n",
      "label example one_hot:  [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "n_records 5000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10368d6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numberKey = 5\n",
    "n_records, n_features = train_features.shape  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "print('number of features: ', n_features)\n",
    "print('label example one_hot: ', train_labels[numberKey])\n",
    "print('n_records', n_records)\n",
    "plt.imshow(train_features[numberKey].reshape((28, 28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = train_labels\n",
    "inputs = train_features\n",
    "n_hidden = 100  # number of hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights\n",
    "We want these to be small such that the input to the sigmoid is in the linear region near 0 and not squashed at the high and low ends. It's also important to initialize them randomly so that they all have different starting values and diverge, breaking symmetry. So, we'll initialize the weights from a normal distribution centered at 0. A good value for the scale is 1/âˆšn where n is the number of input units. This keeps the input to the sigmoid low for increasing numbers of input units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.229306475173\n",
      "Train loss:  0.207432366788\n",
      "Train loss:  0.189163166559\n",
      "Train loss:  0.174049441356\n",
      "Train loss:  0.161604541091\n",
      "Train loss:  0.151364468786\n",
      "Train loss:  0.142919525556\n",
      "Train loss:  0.135924977288\n",
      "Train loss:  0.130099277345\n",
      "Train loss:  0.125216469051\n",
      "Train loss:  0.121096852121\n",
      "Train loss:  0.117598004093\n",
      "Train loss:  0.114607009848\n",
      "Train loss:  0.11203409745\n",
      "Train loss:  0.109807582393\n",
      "Train loss:  0.107869917888\n",
      "Train loss:  0.106174635077\n",
      "Train loss:  0.104683980237\n",
      "Train loss:  0.103367089895\n",
      "Train loss:  0.102198578106\n",
      "Train loss:  0.101157438684\n",
      "Train loss:  0.100226188205\n",
      "Train loss:  0.0993901935027\n",
      "Train loss:  0.0986371410171\n",
      "Train loss:  0.0979566157185\n",
      "Train loss:  0.0973397650767\n",
      "Train loss:  0.0967790293876\n",
      "Train loss:  0.0962679241531\n",
      "Train loss:  0.095800863518\n",
      "Train loss:  0.0953730162654\n",
      "Train loss:  0.0949801877694\n",
      "Train loss:  0.0946187227518\n",
      "Train loss:  0.0942854247962\n",
      "Train loss:  0.0939774894303\n",
      "Train loss:  0.0936924482434\n",
      "Train loss:  0.0934281220244\n",
      "Train loss:  0.0931825813046\n",
      "Train loss:  0.0929541130062\n",
      "Train loss:  0.092741192146\n",
      "Train loss:  0.0925424577432\n",
      "Train loss:  0.092356692234\n",
      "Train loss:  0.092182803826\n",
      "Train loss:  0.0920198113233\n",
      "Train loss:  0.0918668310366\n",
      "Train loss:  0.0917230654585\n",
      "Train loss:  0.0915877934383\n",
      "Train loss:  0.0914603616334\n",
      "Train loss:  0.0913401770539\n",
      "Train loss:  0.091226700541\n",
      "Train loss:  0.0911194410516\n",
      "Train loss:  0.0910179506345\n",
      "Train loss:  0.0909218200068\n",
      "Train loss:  0.0908306746486\n",
      "Train loss:  0.0907441713486\n",
      "Train loss:  0.0906619951424\n",
      "Train loss:  0.0905838565927\n",
      "Train loss:  0.0905094893692\n",
      "Train loss:  0.0904386480911\n",
      "Train loss:  0.0903711063995\n",
      "Train loss:  0.0903066552334\n",
      "Train loss:  0.0902451012843\n",
      "Train loss:  0.0901862656081\n",
      "Train loss:  0.0901299823781\n",
      "Train loss:  0.0900760977611\n",
      "Train loss:  0.0900244689039\n",
      "Train loss:  0.0899749630182\n",
      "Train loss:  0.0899274565521\n",
      "Train loss:  0.0898818344404\n",
      "Train loss:  0.089837989424\n",
      "Train loss:  0.0897958214322\n",
      "Train loss:  0.0897552370206\n",
      "Train loss:  0.0897161488594\n",
      "Train loss:  0.0896784752672\n",
      "Train loss:  0.0896421397851\n",
      "Train loss:  0.0896070707879\n",
      "Train loss:  0.089573201128\n",
      "Train loss:  0.0895404678099\n",
      "Train loss:  0.089508811691\n",
      "Train loss:  0.0894781772079\n",
      "Train loss:  0.0894485121245\n",
      "Train loss:  0.0894197673003\n",
      "Train loss:  0.0893918964773\n",
      "Train loss:  0.0893648560836\n",
      "Train loss:  0.0893386050527\n",
      "Train loss:  0.089313104656\n",
      "Train loss:  0.0892883183486\n",
      "Train loss:  0.0892642116265\n",
      "Train loss:  0.0892407518948\n",
      "Train loss:  0.089217908345\n",
      "Train loss:  0.089195651842\n",
      "Train loss:  0.0891739548188\n",
      "Train loss:  0.089152791179\n",
      "Train loss:  0.0891321362061\n",
      "Train loss:  0.089111966479\n",
      "Train loss:  0.0890922597941\n",
      "Train loss:  0.0890729950919\n",
      "Train loss:  0.0890541523891\n",
      "Train loss:  0.0890357127151\n",
      "Train loss:  0.0890176580533\n",
      "Train loss:  0.0889999712852\n"
     ]
    }
   ],
   "source": [
    "weights = {\n",
    "    'weights_input_hidden': np.random.normal(scale=1/n_features**.5, size=(n_features, n_hidden)),\n",
    "    'weights_hidden_output': np.random.normal(scale=1/n_hidden**.5, size=(n_hidden, n_classes))\n",
    "}\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 100\n",
    "learnrate = 0.05\n",
    "\n",
    "last_loss = None\n",
    "\n",
    "\n",
    "#TODO: implement softmax and cross-entropy\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights['weights_input_hidden'].shape)\n",
    "    del_w_hidden_output = np.zeros(weights['weights_hidden_output'].shape)\n",
    "    for x, y in zip(inputs, targets):\n",
    "        hidden_output = sigmoid(np.dot(x, weights['weights_input_hidden']))\n",
    "        output = sigmoid(np.dot(hidden_output, weights['weights_hidden_output']))\n",
    "        ## Backward pass ##\n",
    "        # Î´o=(y âˆ’ Ã¿)fâ€²(h)\n",
    "        delta_error_output = (y - output) * output * (1 - output)\n",
    "\n",
    "        # Î´h = W * Î´o * fâ€²(h)\n",
    "        delta_hidden_error = np.dot(weights['weights_hidden_output'], delta_error_output) * hidden_output * (1 - hidden_output)\n",
    "        \n",
    "        # TODO: Update the change in weights\n",
    "        #del_w_hidden_output += np.dot(hidden_output[:, None], delta_error_output[None, :])\n",
    "        \n",
    "        del_w_hidden_output += delta_error_output * hidden_output[:, None]\n",
    "        del_w_input_hidden += delta_hidden_error * x[:, None]\n",
    "        \n",
    "    # Update weights\n",
    "    weights['weights_input_hidden'] += learnrate * del_w_input_hidden / n_records\n",
    "    weights['weights_hidden_output'] += learnrate * del_w_hidden_output / n_records\n",
    "    \n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if 1: #e % (epochs / 50) == 0:\n",
    "        hidden_output = sigmoid(np.dot(inputs, weights['weights_input_hidden']))\n",
    "        out = sigmoid(np.dot(hidden_output, weights['weights_hidden_output']))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
